{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "import pyspark\n",
    "from pyspark.ml import feature, regression, Pipeline\n",
    "from pyspark.sql import functions as fn, Row\n",
    "from pyspark import sql\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col,udf\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_Jandata = spark.createDataFrame(pd.read_csv(os.environ.get(\"TAXI_DATA\")))\n",
    "raw_weatherdata = spark.createDataFrame(pd.read_excel(os.environ.get(\"WEATHER_DATA\")))\n",
    "raw_holidaysdata = spark.createDataFrame(pd.read_excel(os.environ.get(\"HOLIDAY_DATA\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_Jandata1=raw_Jandata\n",
    "raw_weatherdata1 = raw_weatherdata\n",
    "raw_holidaysdata1=raw_holidaysdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_Jandata1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_weatherdata1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_holidaysdata1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|             Holiday|   newDate|\n",
      "+--------------------+----------+\n",
      "|       New Years Day|2015-01-01|\n",
      "|Martin Luther Kin...|2015-01-19|\n",
      "|  Lincoln's Birthday|2015-02-12|\n",
      "|     Presidents' Day|2015-02-16|\n",
      "|        Mother's Day|2015-05-10|\n",
      "|        Memorial Day|2015-05-25|\n",
      "|        Father's Day|2015-06-21|\n",
      "|Independence Day ...|2015-07-03|\n",
      "|           Labor Day|2015-09-07|\n",
      "|        Columbus Day|2015-10-12|\n",
      "|        Veterans Day|2015-11-11|\n",
      "|        Thanksgiving|2015-11-26|\n",
      "|Day after Thanksg...|2015-11-27|\n",
      "|       Christmas Day|2015-12-25|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_holidaysdata_2 = raw_holidaysdata1.withColumn(\"newDate\", fn.to_date(fn.col(\"Date\")))\n",
    "raw_holidaysdata_clean= raw_holidaysdata_2.drop(\"Date\")\n",
    "raw_holidaysdata_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_weatherdata.show()\n",
    "#raw_weatherdata1 = raw_weatherdata.withColumn(\"new_pickup_date\", fn.to_date(fn.col(\"pickup_date\")))\n",
    "#raw_weatherdata1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_date</th>\n",
       "      <th>avg_temp_C</th>\n",
       "      <th>Rain</th>\n",
       "      <th>Fog</th>\n",
       "      <th>Snow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-03</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pickup_date  avg_temp_C  Rain  Fog  Snow\n",
       "0  2015-01-01           1     0    0     0\n",
       "1  2015-01-02           4     0    0     0\n",
       "2  2015-01-03           3     1    0     1\n",
       "3  2015-01-04           9     1    0     0\n",
       "4  2015-01-05           2     0    0     0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_weatherdata2 = pd.read_excel(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Weather Data/2015_weather_original.xlsx\")\n",
    "raw_weatherdata2[\"pickup_date\"] = pd.to_datetime(raw_weatherdata2['pickup_date'], format=\"%d.%m.%y\", errors='coerce')\n",
    "raw_weatherdata2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----+---+----+----------+\n",
      "|        pickup_date|avg_temp_C|Rain|Fog|Snow| date_only|\n",
      "+-------------------+----------+----+---+----+----------+\n",
      "|2015-01-01 00:00:00|         1|   0|  0|   0|2015-01-01|\n",
      "|2015-01-02 00:00:00|         4|   0|  0|   0|2015-01-02|\n",
      "|2015-01-03 00:00:00|         3|   1|  0|   1|2015-01-03|\n",
      "|2015-01-04 00:00:00|         9|   1|  0|   0|2015-01-04|\n",
      "|2015-01-05 00:00:00|         2|   0|  0|   0|2015-01-05|\n",
      "|2015-01-06 00:00:00|        -6|   0|  0|   1|2015-01-06|\n",
      "|2015-01-07 00:00:00|        -9|   0|  0|   0|2015-01-07|\n",
      "|2015-01-08 00:00:00|        -9|   0|  0|   0|2015-01-08|\n",
      "|2015-01-09 00:00:00|        -3|   0|  0|   1|2015-01-09|\n",
      "|2015-01-10 00:00:00|        -7|   0|  0|   0|2015-01-10|\n",
      "|2015-01-11 00:00:00|        -2|   0|  0|   0|2015-01-11|\n",
      "|2015-01-12 00:00:00|         3|   1|  0|   0|2015-01-12|\n",
      "|2015-01-13 00:00:00|        -3|   0|  0|   0|2015-01-13|\n",
      "|2015-01-14 00:00:00|        -4|   0|  0|   0|2015-01-14|\n",
      "|2015-01-15 00:00:00|        -1|   0|  0|   0|2015-01-15|\n",
      "|2015-01-16 00:00:00|         0|   0|  0|   0|2015-01-16|\n",
      "|2015-01-17 00:00:00|        -4|   0|  0|   0|2015-01-17|\n",
      "|2015-01-18 00:00:00|         3|   1|  1|   0|2015-01-18|\n",
      "|2015-01-19 00:00:00|         4|   0|  0|   0|2015-01-19|\n",
      "|2015-01-20 00:00:00|         2|   0|  0|   0|2015-01-20|\n",
      "+-------------------+----------+----+---+----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_weatherdata_clean = spark.createDataFrame(raw_weatherdata2)\n",
    "raw_weatherdata_clean = raw_weatherdata_clean.withColumn(\"date_only\", fn.to_date(fn.col(\"pickup_date\")))\n",
    "raw_weatherdata_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_Jan_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-01_100k.csv\")\n",
    "# raw_Jan_Data1[\"pickup_date\"] = pd.to_datetime(raw_Jan_Data1['pickup_datetime'], format=\"%.%m.%y\", errors='coerce')\n",
    "# raw_Jan_Data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------------+---------------+-------------+------------------+------------------+----------+------------------+------------------+-----------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+----------+--------------+--------------------+-----------+---------------+--------------------+----------+\n",
      "|VendorID|    pickup_datetime|   dropoff_datetime|passenger_count|trip_distance|  pickup_longitude|   pickup_latitude|RateCodeID|store_and_fwd_flag| dropoff_longitude| dropoff_latitude|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|pickup_zip|pickup_borough| pickup_neighborhood|dropoff_zip|dropoff_borough|dropoff_neighborhood| date_only|\n",
      "+--------+-------------------+-------------------+---------------+-------------+------------------+------------------+----------+------------------+------------------+-----------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+----------+--------------+--------------------+-----------+---------------+--------------------+----------+\n",
      "|       2|2015-01-06 11:39:29|2015-01-06 11:49:15|              1|         1.78|-73.99961853027342| 40.74359893798828|         1|                 N|-73.99220275878906|40.76401901245117|           2|        9.0|  0.0|    0.5|       0.0|         0.0|                  0.3|         9.8|     10011|     Manhattan| Chelsea and Clinton|      10036|      Manhattan| Chelsea and Clinton|2015-01-06|\n",
      "|       1|2015-01-13 09:18:29|2015-01-13 09:23:40|              1|          2.1|-73.98195648193358| 40.77828979492188|         1|                 N|-73.96217346191406|40.80535507202149|           1|        7.5|  0.0|    0.5|      2.45|         0.0|                  0.3|       10.75|     10023|     Manhattan|     Upper West Side|      10024|      Manhattan|     Upper West Side|2015-01-13|\n",
      "|       2|2015-01-16 07:15:44|2015-01-16 07:26:42|              1|         2.33| -73.9911880493164|40.742225646972656|         1|                 N|-73.98161315917969|40.76845169067383|           1|       10.0|  0.0|    0.5|       1.0|         0.0|                  0.3|        11.8|     10010|     Manhattan|Gramercy Park and...|      10019|      Manhattan| Chelsea and Clinton|2015-01-16|\n",
      "+--------+-------------------+-------------------+---------------+-------------+------------------+------------------+----------+------------------+------------------+-----------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+----------+--------------+--------------------+-----------+---------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Jandf = raw_Jandata.withColumn(\"date_only\", fn.to_date(fn.col(\"pickup_datetime\")))\n",
    "Jandf.limit(20).show(3)\n",
    "Jandf.toPandas().to_csv('myJandata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "| date_only|        pickup_date|\n",
      "+----------+-------------------+\n",
      "|2015-03-09|2015-03-09 00:00:00|\n",
      "|2015-05-19|2015-05-19 00:00:00|\n",
      "|2015-03-06|2015-03-06 00:00:00|\n",
      "|2015-04-09|2015-04-09 00:00:00|\n",
      "|2015-09-02|2015-09-02 00:00:00|\n",
      "|2015-12-22|2015-12-22 00:00:00|\n",
      "|2015-05-10|2015-05-10 00:00:00|\n",
      "|2015-09-28|2015-09-28 00:00:00|\n",
      "|2015-03-12|2015-03-12 00:00:00|\n",
      "|2015-03-16|2015-03-16 00:00:00|\n",
      "|2015-04-01|2015-04-01 00:00:00|\n",
      "|2015-04-24|2015-04-24 00:00:00|\n",
      "|2015-03-11|2015-03-11 00:00:00|\n",
      "|2015-06-15|2015-06-15 00:00:00|\n",
      "|2015-10-16|2015-10-16 00:00:00|\n",
      "|2015-11-02|2015-11-02 00:00:00|\n",
      "|2015-11-27|2015-11-27 00:00:00|\n",
      "|2015-12-31|2015-12-31 00:00:00|\n",
      "|2015-08-01|2015-08-01 00:00:00|\n",
      "|2015-04-19|2015-04-19 00:00:00|\n",
      "+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Merged_data = Jandf.join(raw_weatherdata_clean.select(\"new_pickup_date\"), \"date_only\").show()\n",
    "#Merged_data = raw_weatherdata_clean.join(Jandf.select(\"date_only\"), \"new_pickup_date\")\n",
    "#Merged_data = raw_weatherdata_clean.join(Jandf,on =\"date_only\")\n",
    "Merged_data = raw_weatherdata_clean.join(Jandf, ['date_only'], how='full')\n",
    "Merged_data.select(\"date_only\",\"pickup_date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merged_data.toPandas().to_csv('mycsv_merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100334"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Merged_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Jandf.select(\"date_only\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merged_data.toPandas().isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merged_data.toPandas().head(4)\n",
    "Merged_dataPandas = Merged_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_only</th>\n",
       "      <th>pickup_date</th>\n",
       "      <th>avg_temp_C</th>\n",
       "      <th>Rain</th>\n",
       "      <th>Fog</th>\n",
       "      <th>Snow</th>\n",
       "      <th>VendorID</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>...</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>pickup_zip</th>\n",
       "      <th>pickup_borough</th>\n",
       "      <th>pickup_neighborhood</th>\n",
       "      <th>dropoff_zip</th>\n",
       "      <th>dropoff_borough</th>\n",
       "      <th>dropoff_neighborhood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [date_only, pickup_date, avg_temp_C, Rain, Fog, Snow, VendorID, pickup_datetime, dropoff_datetime, passenger_count, trip_distance, pickup_longitude, pickup_latitude, RateCodeID, store_and_fwd_flag, dropoff_longitude, dropoff_latitude, payment_type, fare_amount, extra, mta_tax, tip_amount, tolls_amount, improvement_surcharge, total_amount, pickup_zip, pickup_borough, pickup_neighborhood, dropoff_zip, dropoff_borough, dropoff_neighborhood]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 31 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Merged_dataPandas[Merged_dataPandas.date_only == \"2015-01-24\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_Jan_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-01_100k.csv\")\n",
    "raw_Feb_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-02_100k.csv\")\n",
    "raw_Mar_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-03_100k.csv\")\n",
    "raw_Apr_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-04_100k.csv\")\n",
    "raw_May_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-05_100k.csv\")\n",
    "raw_Jun_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-06_100k.csv\")\n",
    "raw_Jul_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-07_100k.csv\")\n",
    "raw_Aug_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-08_100k.csv\")\n",
    "raw_Sep_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-09_100k.csv\")\n",
    "raw_Oct_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-10_100k.csv\")\n",
    "raw_Nov_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-11_100k.csv\")\n",
    "raw_Dec_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-12_100k.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [raw_Jan_Data1,raw_Feb_Data1,raw_Mar_Data1,raw_Apr_Data1,raw_May_Data1,raw_Jun_Data1,raw_Jul_Data1,\n",
    "          raw_Aug_Data1,raw_Sep_Data1,raw_Oct_Data1,raw_Nov_Data1,raw_Dec_Data1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jan_Feb_Data = raw_Jan_Data1.union(raw_Feb_Data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mar_Apr_Data = raw_Mar_Data1.union(raw_Apr_Data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#May_Jun_Data = raw_May_Data1.union(raw_Jun_Data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jul_Aug_Data = raw_Jul_Data1.union(raw_Aug_Data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sep_Oct_Data = raw_Sep_Data1.union(raw_Oct_Data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nov_Dec_Data = raw_Nov_Data1.union(raw_Dec_Data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FirstQ = Jan_Feb_Data.union(Mar_Apr_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SecondQ = May_Jun_Data.union(Jul_Aug_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ThirdQ = Sep_Oct_Data.union(Nov_Dec_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FirstQ_SecondQ = FirstQ.union(SecondQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full_Data = FirstQ_SecondQ.union(ThirdQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full_Data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_DataPD = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(Full_DataPD)\n",
    "Full_DataPD1=Full_DataPD\n",
    "Full_DataPD1 = spark.createDataFrame(Full_DataPD1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------------+---------------+-------------+------------------+------------------+----------+------------------+------------------+-----------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+----------+--------------+--------------------+-----------+---------------+--------------------+----------+\n",
      "|VendorID|    pickup_datetime|   dropoff_datetime|passenger_count|trip_distance|  pickup_longitude|   pickup_latitude|RateCodeID|store_and_fwd_flag| dropoff_longitude| dropoff_latitude|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|pickup_zip|pickup_borough| pickup_neighborhood|dropoff_zip|dropoff_borough|dropoff_neighborhood| date_only|\n",
      "+--------+-------------------+-------------------+---------------+-------------+------------------+------------------+----------+------------------+------------------+-----------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+----------+--------------+--------------------+-----------+---------------+--------------------+----------+\n",
      "|       2|2015-01-06 11:39:29|2015-01-06 11:49:15|              1|         1.78|-73.99961853027342| 40.74359893798828|         1|                 N|-73.99220275878906|40.76401901245117|           2|        9.0|  0.0|    0.5|       0.0|         0.0|                  0.3|         9.8|     10011|     Manhattan| Chelsea and Clinton|      10036|      Manhattan| Chelsea and Clinton|2015-01-06|\n",
      "|       1|2015-01-13 09:18:29|2015-01-13 09:23:40|              1|          2.1|-73.98195648193358| 40.77828979492188|         1|                 N|-73.96217346191406|40.80535507202149|           1|        7.5|  0.0|    0.5|      2.45|         0.0|                  0.3|       10.75|     10023|     Manhattan|     Upper West Side|      10024|      Manhattan|     Upper West Side|2015-01-13|\n",
      "|       2|2015-01-16 07:15:44|2015-01-16 07:26:42|              1|         2.33| -73.9911880493164|40.742225646972656|         1|                 N|-73.98161315917969|40.76845169067383|           1|       10.0|  0.0|    0.5|       1.0|         0.0|                  0.3|        11.8|     10010|     Manhattan|Gramercy Park and...|      10019|      Manhattan| Chelsea and Clinton|2015-01-16|\n",
      "+--------+-------------------+-------------------+---------------+-------------+------------------+------------------+----------+------------------+------------------+-----------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+----------+--------------+--------------------+-----------+---------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Full_DataPD2 = Full_DataPD1.withColumn(\"date_only\", fn.to_date(fn.col(\"pickup_datetime\")))\n",
    "Full_DataPD2.limit(20).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merged_FULL_data = raw_weatherdata_clean.join(Full_DataPD2, ['date_only'], how='full')\n",
    "Merged_FULL_data = Full_DataPD2.join(raw_weatherdata_clean, ['date_only'], how='left')\n",
    "Merged_FULL_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merged_FULL_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o125.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 25.0 failed 1 times, most recent failure: Lost task 5.0 in stage 25.0 (TID 507, localhost, executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat java.util.Arrays.copyOf(Arrays.java:3181)\n\tat java.util.ArrayList.trimToSize(ArrayList.java:198)\n\tat net.razorvine.pickle.UnpickleStack.pop_all_since_marker(UnpickleStack.java:44)\n\tat net.razorvine.pickle.Unpickler.load_tuple(Unpickler.java:501)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:242)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:99)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:112)\n\tat org.apache.spark.api.python.SerDeUtil$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(SerDeUtil.scala:188)\n\tat org.apache.spark.api.python.SerDeUtil$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(SerDeUtil.scala:187)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:148)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2775)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2774)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2774)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat java.util.Arrays.copyOf(Arrays.java:3181)\n\tat java.util.ArrayList.trimToSize(ArrayList.java:198)\n\tat net.razorvine.pickle.UnpickleStack.pop_all_since_marker(UnpickleStack.java:44)\n\tat net.razorvine.pickle.Unpickler.load_tuple(Unpickler.java:501)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:242)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:99)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:112)\n\tat org.apache.spark.api.python.SerDeUtil$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(SerDeUtil.scala:188)\n\tat org.apache.spark.api.python.SerDeUtil$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(SerDeUtil.scala:187)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:148)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-83b33588d321>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtraining_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMerged_FULL_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandomSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"# points in training: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"# points in validation: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"# points in testing: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \"\"\"\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o125.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 25.0 failed 1 times, most recent failure: Lost task 5.0 in stage 25.0 (TID 507, localhost, executor driver): java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat java.util.Arrays.copyOf(Arrays.java:3181)\n\tat java.util.ArrayList.trimToSize(ArrayList.java:198)\n\tat net.razorvine.pickle.UnpickleStack.pop_all_since_marker(UnpickleStack.java:44)\n\tat net.razorvine.pickle.Unpickler.load_tuple(Unpickler.java:501)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:242)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:99)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:112)\n\tat org.apache.spark.api.python.SerDeUtil$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(SerDeUtil.scala:188)\n\tat org.apache.spark.api.python.SerDeUtil$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(SerDeUtil.scala:187)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:148)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2775)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2774)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2774)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n\tat java.util.Arrays.copyOf(Arrays.java:3181)\n\tat java.util.ArrayList.trimToSize(ArrayList.java:198)\n\tat net.razorvine.pickle.UnpickleStack.pop_all_since_marker(UnpickleStack.java:44)\n\tat net.razorvine.pickle.Unpickler.load_tuple(Unpickler.java:501)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:242)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:99)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:112)\n\tat org.apache.spark.api.python.SerDeUtil$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(SerDeUtil.scala:188)\n\tat org.apache.spark.api.python.SerDeUtil$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(SerDeUtil.scala:187)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:148)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 49514)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/apsharma/anaconda3/lib/python3.6/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/Users/apsharma/anaconda3/lib/python3.6/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/Users/apsharma/anaconda3/lib/python3.6/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/Users/apsharma/anaconda3/lib/python3.6/socketserver.py\", line 696, in __init__\n",
      "    self.handle()\n",
      "  File \"/Users/apsharma/anaconda3/lib/python3.6/site-packages/pyspark/accumulators.py\", line 265, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/Users/apsharma/anaconda3/lib/python3.6/site-packages/pyspark/accumulators.py\", line 238, in poll\n",
      "    if func():\n",
      "  File \"/Users/apsharma/anaconda3/lib/python3.6/site-packages/pyspark/accumulators.py\", line 242, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/Users/apsharma/anaconda3/lib/python3.6/site-packages/pyspark/serializers.py\", line 692, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# training_df, validation_df, testing_df = Merged_FULL_data.randomSplit([0.6, 0.3, 0.1], seed=0)\n",
    "# print(\"# points in training: \", training_df.count())\n",
    "# print(\"# points in validation: \", validation_df.count())\n",
    "# print(\"# points in testing: \", testing_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_df.write.csv('Training_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation_df.write.csv('Validation_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing_df.write.csv('Testing_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged_FULL_data.write.csv('Complete_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
