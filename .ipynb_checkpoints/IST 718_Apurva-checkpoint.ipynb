{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "import pyspark\n",
    "from pyspark.ml import feature, regression, Pipeline\n",
    "from pyspark.sql import functions as fn, Row\n",
    "from pyspark import sql\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col,udf\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_date</th>\n",
       "      <th>avg_temp_C</th>\n",
       "      <th>Rain</th>\n",
       "      <th>Fog</th>\n",
       "      <th>Snow</th>\n",
       "      <th>Holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-03</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pickup_date  avg_temp_C  Rain  Fog  Snow  Holiday\n",
       "0  2015-01-01           1     0    0     0        1\n",
       "1  2015-01-02           4     0    0     0        0\n",
       "2  2015-01-03           3     1    0     1        0\n",
       "3  2015-01-04           9     1    0     0        0\n",
       "4  2015-01-05           2     0    0     0        0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_weather_holiday_merged = pd.read_excel(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Weather Data/2015_weather_holiday_merged.xlsx\")\n",
    "raw_weather_holiday_merged[\"pickup_date\"] = pd.to_datetime(raw_weather_holiday_merged['pickup_date'], format=\"%d.%m.%y\", errors='coerce')\n",
    "raw_weather_holiday_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----+---+----+-------+----------+\n",
      "|        pickup_date|avg_temp_C|Rain|Fog|Snow|Holiday| date_only|\n",
      "+-------------------+----------+----+---+----+-------+----------+\n",
      "|2015-01-01 00:00:00|         1|   0|  0|   0|      1|2015-01-01|\n",
      "|2015-01-02 00:00:00|         4|   0|  0|   0|      0|2015-01-02|\n",
      "|2015-01-03 00:00:00|         3|   1|  0|   1|      0|2015-01-03|\n",
      "|2015-01-04 00:00:00|         9|   1|  0|   0|      0|2015-01-04|\n",
      "|2015-01-05 00:00:00|         2|   0|  0|   0|      0|2015-01-05|\n",
      "|2015-01-06 00:00:00|        -6|   0|  0|   1|      0|2015-01-06|\n",
      "|2015-01-07 00:00:00|        -9|   0|  0|   0|      0|2015-01-07|\n",
      "|2015-01-08 00:00:00|        -9|   0|  0|   0|      0|2015-01-08|\n",
      "|2015-01-09 00:00:00|        -3|   0|  0|   1|      0|2015-01-09|\n",
      "|2015-01-10 00:00:00|        -7|   0|  0|   0|      0|2015-01-10|\n",
      "|2015-01-11 00:00:00|        -2|   0|  0|   0|      0|2015-01-11|\n",
      "|2015-01-12 00:00:00|         3|   1|  0|   0|      0|2015-01-12|\n",
      "|2015-01-13 00:00:00|        -3|   0|  0|   0|      0|2015-01-13|\n",
      "|2015-01-14 00:00:00|        -4|   0|  0|   0|      0|2015-01-14|\n",
      "|2015-01-15 00:00:00|        -1|   0|  0|   0|      0|2015-01-15|\n",
      "|2015-01-16 00:00:00|         0|   0|  0|   0|      0|2015-01-16|\n",
      "|2015-01-17 00:00:00|        -4|   0|  0|   0|      0|2015-01-17|\n",
      "|2015-01-18 00:00:00|         3|   1|  1|   0|      0|2015-01-18|\n",
      "|2015-01-19 00:00:00|         4|   0|  0|   0|      1|2015-01-19|\n",
      "|2015-01-20 00:00:00|         2|   0|  0|   0|      0|2015-01-20|\n",
      "+-------------------+----------+----+---+----+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_weather_holiday_merged = spark.createDataFrame(raw_weather_holiday_merged)\n",
    "raw_weather_holiday_merged = raw_weather_holiday_merged.withColumn(\"date_only\", fn.to_date(fn.col(\"pickup_date\")))\n",
    "raw_weather_holiday_merged.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_holiday_data_clean = raw_weather_holiday_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_Jan_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-01_100k.csv\")\n",
    "raw_Feb_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-02_100k.csv\")\n",
    "raw_Mar_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-03_100k.csv\")\n",
    "raw_Apr_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-04_100k.csv\")\n",
    "raw_May_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-05_100k.csv\")\n",
    "raw_Jun_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-06_100k.csv\")\n",
    "raw_Jul_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-07_100k.csv\")\n",
    "raw_Aug_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-08_100k.csv\")\n",
    "raw_Sep_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-09_100k.csv\")\n",
    "raw_Oct_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-10_100k.csv\")\n",
    "raw_Nov_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-11_100k.csv\")\n",
    "raw_Dec_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-12_100k.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [raw_Jan_Data1,raw_Feb_Data1,raw_Mar_Data1,raw_Apr_Data1,raw_May_Data1,raw_Jun_Data1,raw_Jul_Data1,\n",
    "          raw_Aug_Data1,raw_Sep_Data1,raw_Oct_Data1,raw_Nov_Data1,raw_Dec_Data1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_DataPD = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_DataPD1=Full_DataPD\n",
    "Full_DataPD1 = spark.createDataFrame(Full_DataPD1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-------------------+---------------+-------------+------------------+------------------+----------+------------------+------------------+-----------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+----------+--------------+--------------------+-----------+---------------+--------------------+----------+\n",
      "|VendorID|    pickup_datetime|   dropoff_datetime|passenger_count|trip_distance|  pickup_longitude|   pickup_latitude|RateCodeID|store_and_fwd_flag| dropoff_longitude| dropoff_latitude|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|pickup_zip|pickup_borough| pickup_neighborhood|dropoff_zip|dropoff_borough|dropoff_neighborhood| date_only|\n",
      "+--------+-------------------+-------------------+---------------+-------------+------------------+------------------+----------+------------------+------------------+-----------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+----------+--------------+--------------------+-----------+---------------+--------------------+----------+\n",
      "|       2|2015-01-06 11:39:29|2015-01-06 11:49:15|              1|         1.78|-73.99961853027342| 40.74359893798828|         1|                 N|-73.99220275878906|40.76401901245117|           2|        9.0|  0.0|    0.5|       0.0|         0.0|                  0.3|         9.8|     10011|     Manhattan| Chelsea and Clinton|      10036|      Manhattan| Chelsea and Clinton|2015-01-06|\n",
      "|       1|2015-01-13 09:18:29|2015-01-13 09:23:40|              1|          2.1|-73.98195648193358| 40.77828979492188|         1|                 N|-73.96217346191406|40.80535507202149|           1|        7.5|  0.0|    0.5|      2.45|         0.0|                  0.3|       10.75|     10023|     Manhattan|     Upper West Side|      10024|      Manhattan|     Upper West Side|2015-01-13|\n",
      "|       2|2015-01-16 07:15:44|2015-01-16 07:26:42|              1|         2.33| -73.9911880493164|40.742225646972656|         1|                 N|-73.98161315917969|40.76845169067383|           1|       10.0|  0.0|    0.5|       1.0|         0.0|                  0.3|        11.8|     10010|     Manhattan|Gramercy Park and...|      10019|      Manhattan| Chelsea and Clinton|2015-01-16|\n",
      "+--------+-------------------+-------------------+---------------+-------------+------------------+------------------+----------+------------------+------------------+-----------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+----------+--------------+--------------------+-----------+---------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Full_DataPD2 = Full_DataPD1.withColumn(\"date_only\", fn.to_date(fn.col(\"pickup_datetime\")))\n",
    "Full_DataPD2.limit(20).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged_FULL_data = weather_holiday_data_clean.join(Full_DataPD2, ['date_only'], how='full') #execute this when needed\n",
    "## Important --  Merged_FULL_data = Full_DataPD2.join(weather_holiday_data_clean, ['date_only'], how='left') #not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df, validation_df, testing_df = Merged_FULL_data.randomSplit([0.6, 0.3, 0.1], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##training_df.write.csv('Training_Data_one.csv') #Saves in Parts !\n",
    "training_df.toPandas().to_csv(\"Training_Data_Final.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##validation_df.write.csv('Validation_Data_one.csv')\n",
    "validation_df.toPandas().to_csv(\"Validation_Data_Final.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##testing_df.write.csv('Testing_Data_one.csv')\n",
    "testing_df.toPandas().to_csv(\"Testing_Data_Final.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# points in Merged_FULL_data:  1200000\n"
     ]
    }
   ],
   "source": [
    "print(\"# points in Merged_FULL_data: \", Merged_FULL_data.count()) #must be 12 Million rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# points in training:  719469\n"
     ]
    }
   ],
   "source": [
    "print(\"# points in training: \", training_df.count()) #must be 719469 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# points in validation:  360063\n"
     ]
    }
   ],
   "source": [
    "print(\"# points in validation: \", validation_df.count()) #must be 360063 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# points in testing:  120468\n"
     ]
    }
   ],
   "source": [
    "print(\"# points in testing: \", testing_df.count()) #must be 120468 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged_FULL_data.write.csv('Merged_FULL_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged_FULL_data.coalesce(1).write.option(\"header\", \"true\").csv(\"Merged_FULL_data_Final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ----------------------START BELOW !--------------------- ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_Data = pd.read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
