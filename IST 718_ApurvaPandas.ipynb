{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "import pyspark\n",
    "from pyspark.ml import feature, regression, Pipeline\n",
    "from pyspark.sql import functions as fn, Row\n",
    "from pyspark import sql\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col,udf\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_weather_holiday_merged = pd.read_excel(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Weather Data/2015_weather_holiday_merged.xlsx\")\n",
    "raw_weather_holiday_merged[\"pickup_date\"] = pd.to_datetime(raw_weather_holiday_merged['pickup_date'], format=\"%d.%m.%y\", errors='coerce')\n",
    "raw_weather_holiday_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_weather_holiday_merged = spark.createDataFrame(raw_weather_holiday_merged)\n",
    "raw_weather_holiday_merged = raw_weather_holiday_merged.withColumn(\"date_only\", fn.to_date(fn.col(\"pickup_date\")))\n",
    "raw_weather_holiday_merged.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_holiday_data_clean = raw_weather_holiday_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_Jan_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-01_100k.csv\")\n",
    "raw_Feb_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-02_100k.csv\")\n",
    "raw_Mar_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-03_100k.csv\")\n",
    "raw_Apr_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-04_100k.csv\")\n",
    "raw_May_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-05_100k.csv\")\n",
    "raw_Jun_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-06_100k.csv\")\n",
    "raw_Jul_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-07_100k.csv\")\n",
    "raw_Aug_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-08_100k.csv\")\n",
    "raw_Sep_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-09_100k.csv\")\n",
    "raw_Oct_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-10_100k.csv\")\n",
    "raw_Nov_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-11_100k.csv\")\n",
    "raw_Dec_Data1 = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/IST 718 Project/Taxi Data/2015-12_100k.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [raw_Jan_Data1,raw_Feb_Data1,raw_Mar_Data1,raw_Apr_Data1,raw_May_Data1,raw_Jun_Data1,raw_Jul_Data1,\n",
    "          raw_Aug_Data1,raw_Sep_Data1,raw_Oct_Data1,raw_Nov_Data1,raw_Dec_Data1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_DataPD = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_DataPD1=Full_DataPD\n",
    "Full_DataPD1 = spark.createDataFrame(Full_DataPD1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Full_DataPD2 = Full_DataPD1.withColumn(\"date_only\", fn.to_date(fn.col(\"pickup_datetime\")))\n",
    "Full_DataPD2.limit(20).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged_FULL_data = weather_holiday_data_clean.join(Full_DataPD2, ['date_only'], how='full') #execute this when needed\n",
    "## Important --  Merged_FULL_data = Full_DataPD2.join(weather_holiday_data_clean, ['date_only'], how='left') #not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df, validation_df, testing_df = Merged_FULL_data.randomSplit([0.6, 0.3, 0.1], seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##training_df.write.csv('Training_Data_one.csv') #Saves in Parts !\n",
    "training_df.toPandas().to_csv(\"Training_Data_Final.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##validation_df.write.csv('Validation_Data_one.csv')\n",
    "validation_df.toPandas().to_csv(\"Validation_Data_Final.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##testing_df.write.csv('Testing_Data_one.csv')\n",
    "testing_df.toPandas().to_csv(\"Testing_Data_Final.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# points in Merged_FULL_data: \", Merged_FULL_data.count()) #must be 12 Million rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# points in training: \", training_df.count()) #must be 719469 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# points in validation: \", validation_df.count()) #must be 360063 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# points in testing: \", testing_df.count()) #must be 120468 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged_FULL_data.write.csv('Merged_FULL_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged_FULL_data.coalesce(1).write.option(\"header\", \"true\").csv(\"Merged_FULL_data_Final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ----------------------START BELOW !--------------------- ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Read_Merged_Data = pd.read_csv(\"/Users/apsharma/IST718 Dropbox/Apurva Sharma/Merged_FULL_data_Final.csv/part-00000-a210c3c0-691a-407f-b775-a630a67467ab-c000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# points in Merged_FULL_data: \", Read_Merged_Data.count()) #must be 12 Million rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Read_Merged_Data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"# points in Read_full_Data: \", Read_Merged_Data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Read_Merged_Data1 = Read_Merged_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Read_Merged_Data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Read_Merged_Data1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df_Sample = Read_Merged_Data1.sample(60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df_Sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################---------VISUALIZATIONS--------------------------------------------------#########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df_Sample.to_csv(\"dummy_df_Sample_Nov9.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged_Data1=dummy_df_Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train dataset start date: \", min(Merged_Data1[\"pickup_datetime\"]))\n",
    "print(\"Train dataset end date: \", max(Merged_Data1[\"pickup_datetime\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged_Data1[\"pickup_datetime\"] = pd.to_datetime(Merged_Data1[\"pickup_datetime\"])\n",
    "Merged_Data1[\"dropoff_datetime\"] = pd.to_datetime(Merged_Data1[\"dropoff_datetime\"])\n",
    "Merged_Data1[\"pickup_datetime\"] = pd.to_datetime(Merged_Data1[\"pickup_datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged_Data1[\"pickup_dayofweek\"] = Merged_Data1.pickup_datetime.dt.dayofweek\n",
    "Merged_Data1[\"pickup_weekday_name\"] = Merged_Data1.pickup_datetime.dt.weekday_name\n",
    "Merged_Data1[\"pickup_hour\"] = Merged_Data1.pickup_datetime.dt.hour\n",
    "Merged_Data1[\"pickup_month\"] = Merged_Data1.pickup_datetime.dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged_Data1.to_csv(\"Merged_Data_Nov9.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# plt.figure(figsize=(12,8))\n",
    "# sns.countplot(x=\"pickup_weekday_name\", data=Read_Merged_Data1)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12,8))\n",
    "# sns.countplot(x=\"pickup_hour\", data=Read_Merged_Data1)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12,8))\n",
    "# sns.countplot(x=\"pickup_month\", data=Read_Merged_Data1)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read_Merged_Data1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read_Merged_Data1.to_csv(\"Read_Merged_Data1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merged_Sample = Read_Merged_Data1.sample(n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merged_Sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merged_Sample.to_csv(\"sanman.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Descriptive STATS\n",
    "# Merged_Sample.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
